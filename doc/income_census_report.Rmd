---
title: "Predicting Income levels of an individual"
author: "Aishwarya Gopal, Fei Chang, Yanhua Chen"
date: "2020/11/29"
always_allow_html: true
output:    
   html_document:
     toc: true
bibliography: income_census_refs.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(knitr)
library(kableExtra)
library(tidyverse)
```

```{r load eda figures, include=FALSE}
age <- "../results/age.png"
work_hours <- "../results/work-hours.png"
```

```{r load model results, include=FALSE}
cross_validate_scores <- read.csv("../results/cross_validate_scores.csv")
confusion_matrix <- read.csv("../results/confusion_matrix.csv")%>%
  column_to_rownames(., var = "X")
colnames(confusion_matrix) <- c("<50k", ">=50k")
rownames(confusion_matrix) <- c("<50k", ">=50k")
classification_report <- read.csv("../results/classification_report.csv")
classification_report$X[1] = c("<50k")
classification_report$X[2] = c(">=50k")
```

# Summary

Here we attempt to build a classification model using the Logistic Regression algorithm which uses a set of features like age, workclass, education etc to classify the income levels of an indivdual into one of the two categories: \>\$50k/year or \<=\$50k/year. The target class >=50k was encoded as 1 and the other class as 0. Our final Logistic Regression model performed well on the test data set. We obtained an f1 score of `r round(classification_report$f1.score[2],2)` and an overall accuracy calculated to be `r round(classification_report$precision[3],2)`. It correctly predicted  the income class of `r confusion_matrix[1, 1] + confusion_matrix[2, 2]` individuals. However it incorrectly predicted  `r confusion_matrix[2, 1] + confusion_matrix[1, 2]` examples. 

# Introduction

"A large income is the best recipe for happiness I ever heard of" quotes the famous English novelist Jane Austen. While it might not be the only recipe, income dictates the standard of living and economic status of an individual. So, we decided to study the income distribution of people with different education levels, years of experience etc. The observations in this data set are classified according to income levels, into two categories(\>\$50k/year or \<=\$50k/year). This data set comprises of numbers from many countries around the world but, about 90% of the data has been collected from the USA. So, we are under the assumption that, the median wage required to lead a life in the USA (at the time this data was collected) i.e. \$50k per year, was chosen as a threshold for classification.

Taking into account, the importance and impact of income levels in determining a nation's growth, this study aims to present meaningful insights regarding the same.

# Methods

## Data

The data set used in this project is of income census created by Ronny Kohavi and Barry Becker. It was sourced from the UCI Machine Learning Repository and can be found [here](https://archive.ics.uci.edu/ml/machine-learning-databases/adult/), specifically [this file](https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data). The data contains information such as age, workclass, education etc. The target variable is income and it is divided into two categories (\<=50K and \>50K). The ultimate aim is to train a classifier to predict the income class.

## Analysis

The logistic regression algorithm was used to build a classification model to predict whether an individual earns \>\$50k/year or \<=\$50k/year. All the variables in original data set except for education.num column was used. The education.num column is just a numerical representation of the education level of an individual. We chose to do one-hot encoding on the education column and use that instead. f1 score was chosen as the desired metric and a 10 fold cross-validation was performed on the train set. The R and Python programming languages [@R; @Python] and the following R and Python packages were used to perform the analysis: docopt [@docopt], knitr [@knitr], tidyverse [@tidyverse], docopt [@docoptpython], os [@Python], Pandas [@mckinney-proc-scipy-2010]. The code used to perform the analysis and create this report can be found here: <https://github.com/UBC-MDS/DSCI_522_Group_18>.

# Results & Discussion

To look at the realationship between some of the predictors and the income class, we plotted the graphs of a few predictors according to the class distribution. In the first plot, we aim to visualise the age distribution for different income levels. From the plot, we can infer that, people in the higher age range, were in the higher income class. Experience surely plays a role in deciding the income level.

```{r age v/s income distribution, echo=FALSE, fig.cap="Figure 1. Comparison of income levels w.r.t. age.", out.width = '100%'}

knitr::include_graphics(age)
```

The relationship between work hours per week and income class is the one that we seek to explore next. Individuals with income \>50k have longer average working hours per week than individuals with \<50k. But there are some individuals who have income \<50k, but have higher work hours.

```{r work-hours v/s income distribution, echo=FALSE, fig.cap="Figure 1. Comparison of income levels w.r.t. average work hours per week.", out.width = '100%'}
knitr::include_graphics(work_hours)
```

We chose to build a simple classification model. To find the model that best predicted whether an individual earns \>50k or \<=50k, we performed 10-fold cross validation using the Logistic regression algorithm and Random Forest Classification. We observed that the Random Forest Classification generates a higher training accuracy and f1 score, however, its advantage in cross-validation scores is not obvious. Meanwhile, the gap between train and cross-validation score is very large in Random Forest Classification, which indicates the model is overfitted. At this point, we decide to use Logistic Regression algorithm to build the prediction model.

```{r present cross validation score, echo=FALSE}
kable(cross_validate_scores, caption = "Table 1. Cross validate scores of model performance on train data.") %>%
  kable_styling(full_width = FALSE)
```

Our prediction model performed quite well on test data, the confusion matrix below indicates it only made `r confusion_matrix[2, 1] + confusion_matrix[1, 2]` mistakes. However, most of the mistake are from the "\>=50K" group.

```{r confusion-matrix, echo=FALSE}
kable(confusion_matrix, caption = "Table 2. Confusion matrix of model performance on test data.") %>%
  kable_styling(full_width = FALSE) %>%
  add_header_above(c(" ", "Reference" = 2)) %>% 
  pack_rows("Predicted", 1, 2)
```

This problem is also reflected by the classification report, that the f1.score of "\>=50k" group is much lower than "\<50k" group. This model is not good enough to yet predict the income status of rich people.

```{r}
kable(classification_report, caption = "Table 3. Classification report of model performance on test data.")%>%
  kable_styling(full_width = FALSE)
```

To further improve this model in future, there are several things we can suggest. First, we could consider to add some hyperparameters to control the machine learning process to get a better model. Second, we could look through the model to see if there are any feature(s) driving to a misclassification and explore whether any feature engineering could be used to help the model better predict on observations that it currently is making mistakes on.

# References
